{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7cb85cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìú Licence d'utilisation\n",
    "\n",
    "Ce document est prot√©g√© sous licence **Creative Commons BY-NC-ND 4.0 International**  \n",
    "üîí **Aucune modification ni r√©utilisation sans autorisation explicite de l'auteur.**\n",
    "\n",
    "- üë§ Auteur : Christie Vassilian  \n",
    "- üì• T√©l√©chargement autoris√© uniquement √† usage p√©dagogique personnel  \n",
    "- üö´ R√©utilisation commerciale ou modification interdite  \n",
    "\n",
    "[![Licence CC BY-NC-ND](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBJ 4 ‚Äî Expliquer un mod√®le sans le conna√Ætre : LIME & SHAP\n",
    "### M√©thodes agnostiques + d√©tection & correction de biais\n",
    "\n",
    "Dans ce dernier TP, nous allons :\n",
    "- expliquer une image sans aucune connaissance du mod√®le utilis√© ;\n",
    "- d√©couvrir deux m√©thodes d'explicabilit√© **agnostiques** :\n",
    "  - **LIME-Image** : segmentation en superpixels + mod√®le local ;\n",
    "  - **SHAP (KernelSHAP)** : valeurs de Shapley appliqu√©es √† l'image ;\n",
    "- d√©tecter un biais pr√©sent dans un mod√®le (ex : main, arri√®re-plan, objet secondaire) ;\n",
    "- corriger ce biais par des techniques simples ;\n",
    "- comparer toutes les explications obtenues pendant les 4 TP.\n",
    "\n",
    "Objectif : comprendre pourquoi l‚Äôexplicabilit√© est essentielle pour la confiance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from lime import lime_image\n",
    "import shap\n",
    "from skimage.segmentation import mark_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT).to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "         [0.485, 0.456, 0.406],\n",
    "         [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : ajouter une image avec biais de contexte (ex : oiseau sur branche, objet dans une main)\n",
    "img_path = \"oiseau_branche.jpg\"  # √† adapter\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_np = np.array(img)\n",
    "\n",
    "plt.imshow(img_np)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Image originale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Questions 1‚Äì2\n",
    "1. Sur cette image, quels √©l√©ments pourraient **d√©tourner l'attention** du mod√®le ?  \n",
    "2. Le contexte (branche, main, arri√®re-plan) peut-il influencer une IA ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(images):\n",
    "    tensors = []\n",
    "    for img_np in images:\n",
    "        img_pil = Image.fromarray(img_np.astype(\"uint8\"))\n",
    "        tensors.append(transform(img_pil))\n",
    "    batch = torch.stack(tensors).to(device)\n",
    "    with torch.no_grad():\n",
    "        preds = model(batch)\n",
    "        probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "    return probs.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()\n",
    "explanation = explainer.explain_instance(\n",
    "    img_np,\n",
    "    classifier_fn=predict,\n",
    "    top_labels=1,\n",
    "    hide_color=0,\n",
    "    num_samples=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp, mask = explanation.get_image_and_mask(\n",
    "    label=explanation.top_labels[0],\n",
    "    positive_only=True,\n",
    "    num_features=5,\n",
    "    hide_rest=False,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_np)\n",
    "plt.title(\"Image originale\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(mark_boundaries(temp, mask))\n",
    "plt.title(\"LIME : superpixels importants\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Questions 3‚Äì6  \n",
    "3. Que repr√©sente chaque superpixel ?  \n",
    "4. LIME met-il en avant l‚Äôobjet (oiseau) ou son contexte (branche) ?  \n",
    "5. Est-ce coh√©rent avec ce qu‚Äôon *attend* d‚Äôune IA ?  \n",
    "6. Si le mod√®le regarde la branche, quel type de biais cela r√©v√®le-t-il ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline tr√®s simple : une petite collection d'images identiques\n",
    "background = np.array([img_np for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_shap = shap.KernelExplainer(predict, background)\n",
    "shap_values = explainer_shap.shap_values(np.array([img_np]), nsamples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_img = shap_values[0][0].mean(axis=2)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_np)\n",
    "plt.title(\"Image originale\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(shap_img, cmap='inferno')\n",
    "plt.title(\"SHAP : importance des pixels\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Questions 7‚Äì10  \n",
    "7. SHAP montre-t-il (comme LIME) que le mod√®le utilise le contexte ?  \n",
    "8. SHAP donne une importance **positive/n√©gative** : comment interpr√©ter cela ?  \n",
    "9. SHAP te para√Æt-il plus ‚Äúfin‚Äù que LIME ? Pourquoi ?  \n",
    "10. Si le mod√®le se trompe, SHAP peut-il aider √† comprendre *pourquoi* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison LIME / SHAP / Grad-CAM / IG\n",
    "\n",
    "- **LIME** : segmentation en superpixels ‚Üí explication simple, locale  \n",
    "- **SHAP** : valeurs de Shapley ‚Üí importance positive/n√©gative  \n",
    "- **Grad-CAM** : zones g√©n√©rales, semi-globale (TP2)  \n",
    "- **IG** : explication fine et stable (TP3)\n",
    "\n",
    "### ‚ùì Questions 11‚Äì12  \n",
    "11. Parmi les 4 m√©thodes, laquelle explique le mieux **le biais** de cette image ?  \n",
    "12. Pourquoi est-il utile de combiner plusieurs explications ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple simple : recadrage (correction partielle du biais)\n",
    "h, w, _ = img_np.shape\n",
    "cropped = img_np[h//6:5*h//6, w//6:5*w//6]  # recadrage central (√† ajuster selon l'image)\n",
    "\n",
    "plt.imshow(cropped)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Image recadr√©e (bias r√©duit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_pred = predict([cropped])\n",
    "cropped_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Questions 13‚Äì15  \n",
    "13. Apr√®s correction, le mod√®le regarde-t-il enfin le bon objet ?  \n",
    "14. Le biais √©tait-il li√© **√† l‚Äôimage** ou **au mod√®le** ?  \n",
    "15. Comment pourrait-on corriger ce biais de mani√®re d√©finitive (donn√©es, entra√Ænement‚Ä¶) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí° Synth√®se finale ‚Äî Pourquoi LIME & SHAP sont essentiels\n",
    "\n",
    "- On peut expliquer un mod√®le **sans le conna√Ætre** ‚Üí m√©thodes agnostiques.  \n",
    "- LIME montre les parties essentielles d‚Äôune image (superpixels).  \n",
    "- SHAP donne l‚Äôimportance positive/n√©gative des pixels.  \n",
    "- Ensemble, elles permettent de :\n",
    "  - d√©tecter des **biais contextuels** ;\n",
    "  - comprendre les erreurs du mod√®le ;\n",
    "  - corriger et am√©liorer les donn√©es d‚Äôentra√Ænement.\n",
    "\n",
    "### Derni√®re question  \n",
    "üëâ Selon toi, quelle m√©thode (LIME, SHAP, IG, Grad-CAM, Occlusion‚Ä¶) te semble **la plus convaincante** pour comprendre une IA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee192ff2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìú Licence d'utilisation\n",
    "\n",
    "Ce document est prot√©g√© sous licence **Creative Commons BY-NC-ND 4.0 International**  \n",
    "üîí **Aucune modification ni r√©utilisation sans autorisation explicite de l'auteur.**\n",
    "\n",
    "- üë§ Auteur : Christie Vassilian  \n",
    "- üì• T√©l√©chargement autoris√© uniquement √† usage p√©dagogique personnel  \n",
    "- üö´ R√©utilisation commerciale ou modification interdite  \n",
    "\n",
    "[![Licence CC BY-NC-ND](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b342f05",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
