{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbc6ea0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìú Licence d'utilisation\n",
    "\n",
    "Ce document est prot√©g√© sous licence **Creative Commons BY-NC-ND 4.0 International**  \n",
    "üîí **Aucune modification ni r√©utilisation sans autorisation explicite de l'auteur.**\n",
    "\n",
    "- üë§ Auteur : Christie Vassilian  \n",
    "- üì• T√©l√©chargement autoris√© uniquement √† usage p√©dagogique personnel  \n",
    "- üö´ R√©utilisation commerciale ou modification interdite  \n",
    "\n",
    "[![Licence CC BY-NC-ND](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n",
    "\n",
    "--- 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd58b0e",
   "metadata": {},
   "source": [
    "\n",
    "# TP : Classification vs D√©tection ‚Äî Comprendre CNN, R-CNN et YOLO\n",
    "\n",
    "Dans ce TP, tu vas comparer trois familles de mod√®les de vision artificielle :\n",
    "\n",
    "- **CNN (classification simple)**\n",
    "- **R-CNN (d√©tection en deux √©tapes)**\n",
    "- **YOLO (d√©tection en une seule passe, temps r√©el)**\n",
    "\n",
    "Tu vas utiliser ces mod√®les sur les **m√™mes images**, pour comprendre :\n",
    "\n",
    "- ce que chaque mod√®le **peut** faire,\n",
    "- ce qu‚Äôil **ne peut pas** faire,\n",
    "- **pourquoi** on utilise YOLO aujourd‚Äôhui pour la d√©tection.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectifs du TP\n",
    "\n",
    "- Comprendre la diff√©rence entre **classification** et **d√©tection d‚Äôobjets**.  \n",
    "- Voir comment un CNN simple atteint ses limites pour la localisation.  \n",
    "- Utiliser un mod√®le de type **R-CNN** pour obtenir des bo√Ætes englobantes.  \n",
    "- Utiliser **YOLO** pour comparer vitesse, efficacit√© et temps r√©el.  \n",
    "- Construire une **synth√®se claire** entre les trois familles de mod√®les.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a330ff",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# üü¶ PARTIE 0 ‚Äî Pr√©paration du Notebook\n",
    "\n",
    "üëâ Dans cette partie, on installe les biblioth√®ques n√©cessaires et on pr√©pare l'environnement.\n",
    "\n",
    "> **Q0.1** : Pourquoi a-t-on besoin de biblioth√®ques pr√©-entra√Æn√©es pour ce TP ?  \n",
    "> **Q0.2** : Quels sont les avantages d‚Äôutiliser des mod√®les d√©j√† entra√Æn√©s (comme ResNet ou YOLO) ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866c5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Installation (√† ex√©cuter dans Colab ou un environnement avec internet)\n",
    "# Si tu es sur Google Colab, v√©rifie que tu as bien un GPU (Menu : Ex√©cution > Modifier le type d'ex√©cution > GPU).\n",
    "\n",
    "!pip install ultralytics -q\n",
    "\n",
    "import torch, torchvision\n",
    "print(\"Version torch :\", torch.__version__)\n",
    "print(\"GPU disponible :\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153628ac",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# üü¶ PARTIE 1 ‚Äî CNN : Classification d‚Äôune image enti√®re\n",
    "\n",
    "Un **CNN de classification** prend une image en entr√©e et renvoie **une seule √©tiquette** (chien, chat, voiture‚Ä¶).  \n",
    "Il ne renvoie **aucune information de position** des objets dans l‚Äôimage.\n",
    "\n",
    "> **Q1.1** : En une phrase, explique la diff√©rence entre *classification* et *d√©tection d‚Äôobjets*.  \n",
    "> **Q1.2** : Pourquoi la d√©tection d‚Äôobjets est-elle un probl√®me plus complexe que la simple classification ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c29fc",
   "metadata": {},
   "source": [
    "\n",
    "## 1.1 Chargement d‚Äôun mod√®le de classification (ResNet18)\n",
    "\n",
    "On va utiliser **ResNet18**, un r√©seau de neurones convolutif pr√©-entra√Æn√© sur le jeu de donn√©es ImageNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fbcc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"P√©riph√©rique utilis√© :\", device)\n",
    "\n",
    "# Chargement du mod√®le de classification\n",
    "model_cnn = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model_cnn.eval().to(device)\n",
    "\n",
    "# Pr√©traitement associ√© au mod√®le\n",
    "preprocess = models.ResNet18_Weights.DEFAULT.transforms()\n",
    "labels = models.ResNet18_Weights.DEFAULT.meta[\"categories\"]\n",
    "\n",
    "print(\"Mod√®le ResNet18 charg√© avec succ√®s.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b58273",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Fonction de classification d‚Äôune image\n",
    "\n",
    "On d√©finit une fonction qui :\n",
    "\n",
    "1. charge l‚Äôimage,\n",
    "2. applique les pr√©traitements,\n",
    "3. passe l‚Äôimage dans le r√©seau,\n",
    "4. affiche les **5 classes les plus probables**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_image(path):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    x = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_cnn(x)\n",
    "        probs = torch.softmax(logits, dim=1)[0]\n",
    "\n",
    "    topk = torch.topk(probs, 5)\n",
    "    print(\"Top 5 pr√©dictions :\")\n",
    "    for score, idx in zip(topk.values, topk.indices):\n",
    "        print(f\"{labels[idx]} : {score:.3f}\")\n",
    "\n",
    "    display(img)\n",
    "\n",
    "print(\"Fonction classify_image pr√™te.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b15de",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3 Tester la classification sur une image\n",
    "\n",
    "1. T√©l√©verse une image contenant **un seul objet principal** (par exemple : un chien, une voiture...).  \n",
    "2. Observe la pr√©diction du CNN.\n",
    "\n",
    "> **Q1.3** : Que renvoie exactement le r√©seau (type de sortie) ?  \n",
    "> **Q1.4** : Est-ce que le r√©seau donne une information sur la **position** de l‚Äôobjet dans l‚Äôimage ? Explique.  \n",
    "> **Q1.5** : Quel serait le probl√®me si l‚Äôimage contenait **plusieurs objets diff√©rents** (ex : une personne + un chien + une voiture) ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267820af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# T√©l√©versement d'une image (Google Colab)\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Choisis une image sur ton ordinateur (format .jpg ou .png).\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "image_path = list(uploaded.keys())[0]\n",
    "print(\"Image choisie :\", image_path)\n",
    "\n",
    "classify_image(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e274287d",
   "metadata": {},
   "source": [
    "\n",
    "> **Q1.6** : Explique, en quelques phrases, pourquoi un CNN de classification ne peut pas r√©soudre le probl√®me de **d√©tection d‚Äôobjets**.  \n",
    "> (Indice : une seule pr√©diction pour toute l‚Äôimage, aucune coordonn√©e de bo√Æte...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5a30f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# üü© PARTIE 2 ‚Äî R-CNN : D√©tection en deux √©tapes\n",
    "\n",
    "Les mod√®les de type **R-CNN** sont con√ßus pour la **d√©tection d‚Äôobjets** : ils renvoient des **bo√Ætes englobantes** et des **classes** pour chaque objet d√©tect√©.\n",
    "\n",
    "Id√©e g√©n√©rale (simplifi√©e) :  \n",
    "1. Proposer des **r√©gions candidates** (zones o√π il pourrait y avoir un objet),  \n",
    "2. Appliquer un CNN sur chaque r√©gion,  \n",
    "3. Pr√©dire la **classe** et affiner la **bo√Æte** pour chaque objet.\n",
    "\n",
    "Dans ce TP, on va utiliser un mod√®le pratique : **Faster R-CNN** (impl√©ment√© dans `torchvision`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6056a6d7",
   "metadata": {},
   "source": [
    "\n",
    "## 2.1 Chargement de Faster R-CNN\n",
    "\n",
    "On charge un mod√®le pr√©-entra√Æn√© sur le jeu de donn√©es COCO.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d86eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "model_rcnn = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\").to(device)\n",
    "model_rcnn.eval()\n",
    "\n",
    "print(\"Mod√®le Faster R-CNN charg√©.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7be789",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2 Fonction de d√©tection avec R-CNN\n",
    "\n",
    "On applique le mod√®le sur la **m√™me image** que dans la Partie 1 et on affiche des bo√Ætes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "\n",
    "def detect_rcnn(path, score_threshold=0.5):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_rcnn(x)[0]\n",
    "\n",
    "    boxes = outputs[\"boxes\"].cpu().numpy()\n",
    "    labels_rcnn = outputs[\"labels\"].cpu().numpy()\n",
    "    scores = outputs[\"scores\"].cpu().numpy()\n",
    "\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    for (box, label, score) in zip(boxes, labels_rcnn, scores):\n",
    "        if score < score_threshold:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        cv2.rectangle(img_np, (x1, y1), (x2, y2), 2)\n",
    "        cv2.putText(img_np, f\"{label} ({score:.2f})\", (x1, max(y1-10, 0)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2, 1)\n",
    "\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "print(\"Fonction detect_rcnn pr√™te.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee7c0b",
   "metadata": {},
   "source": [
    "\n",
    "## 2.3 D√©tection sur la m√™me image qu‚Äôen Partie 1\n",
    "\n",
    "On applique maintenant Faster R-CNN sur la **m√™me image** que celle utilis√©e avec le CNN de classification.\n",
    "\n",
    "> **Q2.1** : Combien d‚Äôobjets semblent √™tre d√©tect√©s sur l‚Äôimage ?  \n",
    "> **Q2.2** : Quelles informations suppl√©mentaires obtiens-tu par rapport au CNN de la Partie 1 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detect_rcnn(image_path, score_threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6557c33",
   "metadata": {},
   "source": [
    "\n",
    "> **Q2.3** : D√©cris la forme de la sortie de Faster R-CNN (types d'informations).  \n",
    "> **Q2.4** : En quoi cette sortie est-elle fondamentalement diff√©rente de celle du CNN de classification ?  \n",
    "> **Q2.5** : Pourquoi ce type de mod√®le est-il plus **lent** qu‚Äôun CNN simple (m√™me si on ne voit pas forc√©ment la diff√©rence sur une seule image) ?  \n",
    "> **Q2.6** : Selon toi, quel probl√®me peut se poser si on veut utiliser ce mod√®le en **temps r√©el** sur une webcam ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0ada9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# üüß PARTIE 3 ‚Äî YOLO : D√©tection en une seule passe (temps r√©el)\n",
    "\n",
    "Les mod√®les **YOLO (You Only Look Once)** sont aussi des r√©seaux convolutifs, mais organis√©s pour faire la d√©tection en **une seule passe** sur l‚Äôimage compl√®te.\n",
    "\n",
    "Id√©e cl√© :  \n",
    "- l‚Äôimage est divis√©e en une **grille**,  \n",
    "- le r√©seau pr√©dit, pour chaque zone, des **bo√Ætes + classes**,  \n",
    "- tout est calcul√© en une seule fois, ce qui permet la **vitesse** et le **temps r√©el**.\n",
    "\n",
    "> **Q3.1** : Pourquoi l‚Äôid√©e de ‚Äúregarder l‚Äôimage une seule fois‚Äù (You Only Look Once) est-elle int√©ressante pour le temps r√©el ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65bf50",
   "metadata": {},
   "source": [
    "\n",
    "## 3.1 Chargement du mod√®le YOLOv8\n",
    "\n",
    "On utilise la biblioth√®que `ultralytics` pour charger un mod√®le YOLOv8 pr√©-entra√Æn√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Mod√®le YOLOv8 \"small\" (compromis vitesse/pr√©cision)\n",
    "model_yolo = YOLO(\"yolov8s.pt\")\n",
    "print(\"Mod√®le YOLOv8s charg√©.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8909241",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 D√©tection avec YOLO sur la m√™me image\n",
    "\n",
    "On applique YOLO sur la **m√™me image** que dans les parties 1 et 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_yolo(path):\n",
    "    results = model_yolo(path)\n",
    "    for r in results:\n",
    "        img_plot = r.plot()  # numpy (BGR)\n",
    "        img_plot = cv2.cvtColor(img_plot, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_plot)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "print(\"Fonction detect_yolo pr√™te.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detect_yolo(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2b63e",
   "metadata": {},
   "source": [
    "\n",
    "> **Q3.2** : YOLO renvoie-t-il, lui aussi, des bo√Ætes englobantes et des classes pour plusieurs objets ?  \n",
    "> **Q3.3** : Par rapport √† Faster R-CNN, que remarques-tu sur la ‚Äúr√©activit√©‚Äù du mod√®le (m√™me si on ne fait qu‚Äôune image ici) ?  \n",
    "> **Q3.4** : Pourquoi YOLO est-il particuli√®rement adapt√© √† une utilisation **sur webcam** ou **sur Raspberry Pi** ?  \n",
    "> **Q3.5** : YOLO utilise-t-il encore des convolutions comme les CNN classiques ?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3400960",
   "metadata": {},
   "source": [
    "\n",
    "### (Optionnel) 3.3 YOLO sur la webcam\n",
    "\n",
    "Cette partie n√©cessite un acc√®s cam√©ra (souvent plus simple en local que sur Colab).\n",
    "\n",
    "```python\n",
    "# Exemple indicatif (peut ne pas fonctionner directement sur Colab)\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    results = model_yolo(frame)\n",
    "    plotted = results[0].plot()\n",
    "    cv2.imshow(\"YOLOv8 Webcam\", plotted)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51ec72",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# üü® PARTIE 4 ‚Äî Synth√®se Finale\n",
    "\n",
    "Dans cette derni√®re partie, tu vas comparer les trois familles de mod√®les et r√©diger une courte conclusion.\n",
    "\n",
    "## 4.1 Tableau comparatif (√† compl√©ter)\n",
    "\n",
    "Recopie et compl√®te le tableau suivant dans ton cahier ou dans une cellule texte :\n",
    "\n",
    "| Mod√®le | T√¢che principale | Sortie | Avantages | Limites |\n",
    "|--------|------------------|--------|-----------|---------|\n",
    "| CNN    |                  |        |           |         |\n",
    "| R-CNN  |                  |        |           |         |\n",
    "| YOLO   |                  |        |           |         |\n",
    "\n",
    "> **Q4.1** : Pour chaque mod√®le, pr√©cise s‚Äôil fait de la **classification** ou de la **d√©tection**, et ce qu‚Äôil renvoie exactement en sortie.\n",
    "\n",
    "## 4.2 Questions de synth√®se\n",
    "\n",
    "> **Q4.2** : Explique ce que permet un **CNN**, un **R-CNN** puis un **YOLO**, et les limites de chacun.  \n",
    "> **Q4.3** : Dans quel cas pr√©cis utiliserais-tu :  \n",
    "> - un CNN simple ?  \n",
    "> - un mod√®le de type R-CNN ?  \n",
    "> - un mod√®le de type YOLO ?  \n",
    "\n",
    "> **Q4.4** : Si tu devais cr√©er une application de d√©tection d‚Äôobjets **en temps r√©el** (par exemple pour suivre des objets avec une webcam), quel mod√®le choisirais-tu et pourquoi ?\n",
    "\n",
    "\n",
    "üëè Bravo, tu as maintenant manipul√© trois grandes familles de mod√®les de vision et compris leurs diff√©rences fondamentales :\n",
    "- **CNN** : reconna√Æt *quoi* il y a dans une image.  \n",
    "- **R-CNN** : reconna√Æt *quoi* et *o√π*, mais de mani√®re plus lourde.  \n",
    "- **YOLO** : reconna√Æt *quoi* et *o√π* en **temps r√©el**, en une seule passe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ea1480",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìú Licence d'utilisation\n",
    "\n",
    "Ce document est prot√©g√© sous licence **Creative Commons BY-NC-ND 4.0 International**  \n",
    "üîí **Aucune modification ni r√©utilisation sans autorisation explicite de l'auteur.**\n",
    "\n",
    "- üë§ Auteur : Christie Vassilian  \n",
    "- üì• T√©l√©chargement autoris√© uniquement √† usage p√©dagogique personnel  \n",
    "- üö´ R√©utilisation commerciale ou modification interdite  \n",
    "\n",
    "[![Licence CC BY-NC-ND](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/)\n",
    "\n",
    "--- 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f1d6a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
