{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ecc60e",
   "metadata": {},
   "source": [
    "### üìú Licence d'utilisation\n",
    "Ce notebook est mis √† disposition sous licence **Creative Commons Attribution - Pas d‚ÄôUtilisation Commerciale - Partage dans les M√™mes Conditions 4.0 International**.\n",
    "\n",
    "**Auteur** : Christie Vassilian\n",
    "\n",
    "Vous √™tes libre de le r√©utiliser, de le modifier et de le partager, √† condition de citer l‚Äôauteur original et de ne pas en faire un usage commercial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657b44b",
   "metadata": {},
   "source": [
    "# üåü Fonctions d‚Äôactivation dans les r√©seaux de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8ba05",
   "metadata": {},
   "source": [
    "## üéØ Objectifs p√©dagogiques\n",
    "- Comprendre ce qu‚Äôest une fonction d‚Äôactivation et son r√¥le dans un r√©seau de neurones.\n",
    "- Revoir et mobiliser des notions math√©matiques (fonctions, d√©riv√©es, limites, variations).\n",
    "- S‚Äôinitier √† la programmation Python appliqu√©e √† l‚Äôintelligence artificielle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e5497",
   "metadata": {},
   "source": [
    "## üß† Partie 1 - Qu‚Äôest-ce qu‚Äôun r√©seau de neurones et une fonction d'activation ?\n",
    "\n",
    "Un r√©seau de neurones artificiels est un mod√®le math√©matique inspir√© du cerveau humain. Il est constitu√© de couches de **neurones**, qui effectuent chacun un **calcul simple**.\n",
    "\n",
    "Un **neurone** artificiel re√ßoit des entr√©es (des nombres), les pond√®re (multiplie par des coefficients), les additionne, puis applique une **fonction d‚Äôactivation** √† ce total pour produire une sortie.\n",
    "\n",
    "Sans fonction d‚Äôactivation, le r√©seau ne pourrait **pas apprendre des relations complexes** entre les donn√©es. Les fonctions d‚Äôactivation introduisent **de la non-lin√©arit√©**, ce qui permet au r√©seau d‚Äôapproximer pratiquement n‚Äôimporte quelle fonction.\n",
    "\n",
    "### üîé Partie 1 - Questions :\n",
    "1. ‚úçÔ∏è *Qu‚Äôest-ce qu‚Äôun neurone artificiel ?*\n",
    "2. ‚úçÔ∏è *Pourquoi utilise-t-on une fonction d‚Äôactivation ?*\n",
    "3. ‚úçÔ∏è *Qu‚Äôest-ce qu‚Äôune fonction non-lin√©aire ? Donne un exemple math√©matique simple.*\n",
    "4. ‚úçÔ∏è *Qu‚Äôest-ce que la r√©tropropagation dans un r√©seau de neurones ? Quel lien avec la d√©riv√©e de la fonction d‚Äôactivation ?*\n",
    "5. ‚úçÔ∏è *Pourquoi utilise-t-on une fonction diff√©rente en sortie du r√©seau selon le probl√®me trait√© ? (ex : binaire vs multi-classes)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12c37f",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Partie 2 - Fonction Sigmo√Øde\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431dec8",
   "metadata": {},
   "source": [
    "La fonction sigmo√Øde est une fonction en forme de S, tr√®s utilis√©e pour produire des sorties entre 0 et 1.  \n",
    "Elle est d√©finie par : $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Elle permet d‚Äôinterpr√©ter la sortie comme une probabilit√©. Elle est souvent utilis√©e en **sortie de r√©seau** pour les probl√®mes de **classification binaire**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865432ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Fonction sigmo√Øde\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a99756",
   "metadata": {},
   "source": [
    "### ‚ùì Partie 2 - Questions\n",
    "1. ‚úçÔ∏è *Calcule la valeur de œÉ(0), œÉ(2), et œÉ(-2). Que remarques-tu ?*\n",
    "2. ‚úçÔ∏è *Montre que la fonction est strictement croissante. Pourquoi est-ce important en IA ?*\n",
    "3. ‚úçÔ∏è *Quelle est la limite de œÉ(x) quand x tend vers -‚àû et +‚àû ?*\n",
    "4. ‚úçÔ∏è *Quelle est la d√©riv√©e de œÉ(x) ? Que vaut-elle pour x = 0 ?*\n",
    "5. ‚úçÔ∏è *Pourquoi l‚Äôutilisation de la sigmo√Øde peut poser probl√®me dans les couches profondes ?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85f3d1",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Partie 3 - Fonction ReLU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76c728",
   "metadata": {},
   "source": [
    "La fonction ReLU (Rectified Linear Unit) est d√©finie par :  \n",
    "$\\text{ReLU}(x) = \\max(0, x)$\n",
    "\n",
    "Elle est tr√®s utilis√©e dans les **couches cach√©es** des r√©seaux profonds car elle est simple, rapide et permet une meilleure propagation du gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a295ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = relu(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Fonction ReLU\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2b364",
   "metadata": {},
   "source": [
    "### ‚ùì Partie 3 - Questions\n",
    "1. ‚úçÔ∏è *Trace la courbe de ReLU. Quelle est sa pente √† droite de 0 ? Et √† gauche ?*\n",
    "2. ‚úçÔ∏è *Que vaut la d√©riv√©e de ReLU pour x > 0 ? Pour x < 0 ?*\n",
    "3. ‚úçÔ∏è *Pourquoi la ReLU permet-elle un apprentissage plus rapide que la sigmo√Øde ?*\n",
    "4. ‚úçÔ∏è *En quoi consiste le ph√©nom√®ne de 'neurones morts' ?*\n",
    "5. ‚úçÔ∏è *Impl√©mente ReLU en Python sur x = [-2, -1, 0, 1, 2]. Que remarques-tu ?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b2913",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Partie 4 - Fonction Leaky ReLU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359959a",
   "metadata": {},
   "source": [
    "La fonction Leaky ReLU introduit une l√©g√®re pente sur les valeurs n√©gatives :  \n",
    "$ \\text{LeakyReLU}(x) = \\max(\\varepsilon x, x) $\n",
    "\n",
    "Cela permet de **limiter le probl√®me des neurones morts** rencontr√© avec ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c879eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, epsilon=0.1):\n",
    "    return np.where(x > 0, x, epsilon * x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = leaky_relu(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Fonction Leaky ReLU\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411b2ff",
   "metadata": {},
   "source": [
    "### ‚ùì Partie 4 - Questions\n",
    "1. ‚úçÔ∏è *Trace Leaky ReLU pour Œµ = 0.1. Compare avec ReLU.*\n",
    "2. ‚úçÔ∏è *Que vaut la d√©riv√©e de Leaky ReLU pour x < 0 et x > 0 ?*\n",
    "3. ‚úçÔ∏è *Explique comment Leaky ReLU limite le probl√®me des neurones morts.*\n",
    "4. ‚úçÔ∏è *Est-elle continue et d√©rivable en x = 0 ?*\n",
    "5. ‚úçÔ∏è *Modifie la valeur de Œµ (ex : 0.01, 0.5). Que remarques-tu ?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd78305",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Partie 5 -Fonction Swish\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebb682",
   "metadata": {},
   "source": [
    "La fonction Swish est d√©finie par :  \n",
    "$\\text{swish}(x) = x \\cdot \\sigma(x)$\n",
    "\n",
    "Elle est **continue, d√©rivable** et peut mieux fonctionner que ReLU sur certains r√©seaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76707c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = swish(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Fonction Swish\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b675e3",
   "metadata": {},
   "source": [
    "### ‚ùì Partie 5 - Questions\n",
    "1. ‚úçÔ∏è *Calcule Swish(-2), Swish(0), Swish(2).*\n",
    "2. ‚úçÔ∏è *Trace sa courbe et compare avec ReLU.*\n",
    "3. ‚úçÔ∏è *Pourquoi dit-on que Swish est une fonction 'douce' (smooth) ?*\n",
    "4. ‚úçÔ∏è *Quels sont ses avantages en IA par rapport √† ReLU ?*\n",
    "5. ‚úçÔ∏è *Impl√©mente Swish et observe sa forme compar√©e aux autres fonctions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5ac83",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Partie 6 - Fonction Softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f8c00f",
   "metadata": {},
   "source": [
    "La fonction softmax transforme un vecteur de nombres en **probabilit√©s** dont la somme vaut 1. Elle est utilis√©e **en sortie de r√©seau de classification multi-classes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f549b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # pour la stabilit√© num√©rique\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "y = softmax(x)\n",
    "print(\"R√©sultat de softmax([1, 2, 3]) :\", y)\n",
    "print(\"Somme des sorties :\", np.sum(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a0e40",
   "metadata": {},
   "source": [
    "### ‚ùì Partie 6 - Questions\n",
    "1. ‚úçÔ∏è *Calcule √† la main la softmax de (1, 2, 3). Que vaut la somme ?*\n",
    "2. ‚úçÔ∏è *Que se passe-t-il avec (100, 100, 100) ?*\n",
    "3. ‚úçÔ∏è *Pourquoi softmax est-elle r√©serv√©e aux couches de sortie ?*\n",
    "4. ‚úçÔ∏è *Quelle diff√©rence avec la sigmo√Øde ?*\n",
    "5. ‚úçÔ∏è *Impl√©mente softmax et teste plusieurs vecteurs. Que constates-tu ?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd900114",
   "metadata": {},
   "source": [
    "## üßæ Partie 7 - Bilan p√©dagogique\n",
    "\n",
    "Cette activit√© a permis de :\n",
    "\n",
    "- Comprendre ce qu‚Äôest une fonction d‚Äôactivation et ses usages en IA.\n",
    "- Travailler les notions math√©matiques : fonctions, d√©riv√©es, limites, continuit√©.\n",
    "- Manipuler Python de mani√®re guid√©e pour tracer et analyser des fonctions d‚Äôactivation.\n",
    "\n",
    "### üìå R√©sum√© des cas d‚Äôusage des fonctions d‚Äôactivation\n",
    "\n",
    "| Fonction      | Usage principal en IA                             | Sp√©cificit√©                                      |\n",
    "|---------------|---------------------------------------------------|--------------------------------------------------|\n",
    "| Sigmo√Øde      | Sortie binaire (0 ou 1)                           | Lisse, saturante, interpr√©tation probabiliste    |\n",
    "| ReLU          | Couches cach√©es                                   | Simple, efficace, rapide mais neurones morts     |\n",
    "| Leaky ReLU    | Couches cach√©es                                   | Variante de ReLU qui √©vite les neurones morts    |\n",
    "| Swish         | Couches cach√©es profondes                         | Lisse, performant, nouvelle g√©n√©ration           |\n",
    "| Softmax       | Sortie multi-classes                              | Donne une distribution de probabilit√©            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453060a",
   "metadata": {},
   "source": [
    "## ‚úÖ Partie 8 - Partie professeur ‚Äì R√©ponses d√©taill√©es\n",
    "\n",
    "Voici les r√©ponses comment√©es √† toutes les questions pos√©es dans le notebook, avec le code Python n√©cessaire lorsque c‚Äôest utile pour visualiser ou v√©rifier les r√©sultats.\n",
    "\n",
    "L‚Äôobjectif est que l‚Äô√©l√®ve puisse **travailler en autonomie**, v√©rifier sa compr√©hension et apprendre de ses erreurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7e8fe",
   "metadata": {},
   "source": [
    "### üîπ Fonction Sigmo√Øde ‚Äì R√©ponses\n",
    "\n",
    "1. **Valeurs** :\n",
    "$\n",
    "   \\sigma(0) = \\dfrac{1}{1 + e^{0}} = 0.5, \\quad\n",
    "   \\sigma(2) \\approx 0.88, \\quad\n",
    "   \\sigma(-2) \\approx 0.12\n",
    "$\n",
    "\n",
    "3. **Fonction strictement croissante** : La d√©riv√©e $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$ est strictement positive sur $\\mathbb{R}$, donc la fonction est strictement croissante.\n",
    "\n",
    "4. **Limites** : $$\\lim_{x \\to -\\infty} \\sigma(x) = 0$$ et $$\\lim_{x \\to +\\infty} \\sigma(x) = 1$$\n",
    "\n",
    "6. **D√©riv√©e en x = 0** : $\\sigma(0) = 0.5 \\Rightarrow \\sigma'(0) = 0.5 \\times (1 - 0.5) = 0.25$\n",
    "\n",
    "7. **Probl√®me en r√©seau profond** : Lorsque \\( x \\) devient grand ou petit, la d√©riv√©e de la sigmo√Øde tend vers 0. Ce ph√©nom√®ne est appel√© **vanishing gradient** : il emp√™che les couches profondes d‚Äôapprendre efficacement.\n",
    "\n",
    "#### üîé Code de v√©rification :\n",
    "```python\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "x_vals = np.array([-2, 0, 2])\n",
    "print(sigmoid(x_vals))  # Donne environ [0.12, 0.5, 0.88]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d7b69",
   "metadata": {},
   "source": [
    "### üîπ Fonction ReLU ‚Äì R√©ponses\n",
    "\n",
    "1. **Pente** :\n",
    "   - Pour \\( x > 0 \\), la pente est 1\n",
    "   - Pour \\( x < 0 \\), la pente est 0\n",
    "   - En x = 0, la d√©riv√©e n‚Äôest pas d√©finie (discontinuit√© de la pente)\n",
    "\n",
    "2. **D√©riv√©e** :\n",
    "   $\\text{ReLU}'(x) = \\begin{cases}\n",
    "   1 & \\text{si } x > 0 \\\\\n",
    "   0 & \\text{si } x < 0\n",
    "   \\end{cases}$\n",
    "\n",
    "3. **Efficacit√©** : la ReLU permet un apprentissage rapide car elle ne sature pas (contrairement √† la sigmo√Øde) et garde des d√©riv√©es constantes pour les valeurs positives.\n",
    "\n",
    "4. **Neurones morts** : un neurone est \"mort\" si sa sortie est toujours 0 (entr√©e n√©gative constante), ce qui g√®le son apprentissage.\n",
    "\n",
    "5. **Code Python simple** :\n",
    "```python\n",
    "import numpy as np\n",
    "def relu(x): return np.maximum(0, x)\n",
    "relu(np.array([-2, -1, 0, 1, 2]))  # Renvoie [0 0 0 1 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c04252",
   "metadata": {},
   "source": [
    "### üîπ Fonction Leaky ReLU ‚Äì R√©ponses\n",
    "\n",
    "1. Leaky ReLU pour $\\varepsilon = 0.1$ :\n",
    "   - identique √† ReLU pour $x > 0$\n",
    "   - pente faible $\\varepsilon$ pour $x < 0$\n",
    "\n",
    "2. **D√©riv√©e** :\n",
    "   $\\text{LeakyReLU}'(x) = \\begin{cases}\n",
    "   1 & \\text{si } x > 0 \\\\\n",
    "   \\varepsilon & \\text{si } x < 0\n",
    "   \\end{cases}$\n",
    "\n",
    "3. **Neurones morts** : elle les √©vite car m√™me les valeurs n√©gatives produisent une petite d√©riv√©e.\n",
    "\n",
    "4. **Continuit√© / d√©rivabilit√©** : elle est continue et d√©rivable sur $\\mathbb{R}$.\n",
    "\n",
    "5. **Code Python** :\n",
    "```python\n",
    "def leaky_relu(x, epsilon=0.1):\n",
    "    return np.where(x > 0, x, epsilon * x)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, leaky_relu(x, 0.01), label='Œµ=0.01')\n",
    "plt.plot(x, leaky_relu(x, 0.1), label='Œµ=0.1')\n",
    "plt.plot(x, leaky_relu(x, 0.5), label='Œµ=0.5')\n",
    "plt.legend(); plt.grid(); plt.title(\"Comparaison Leaky ReLU\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ad290",
   "metadata": {},
   "source": [
    "### üîπ Fonction Swish ‚Äì R√©ponses\n",
    "\n",
    "1. **Calculs** :\n",
    "   - Swish(-2) ‚âà -0.24\n",
    "   - Swish(0) = 0\n",
    "   - Swish(2) ‚âà 1.76\n",
    "\n",
    "2. **Forme** : douce, lisse, non lin√©aire. Similaire √† ReLU pour $x > 0$, mais plus progressive autour de $0$.\n",
    "\n",
    "3. **Smooth** : Swish est d√©rivable partout avec une d√©riv√©e continue.\n",
    "\n",
    "4. **Avantage IA** : moins de neurones morts, meilleure capacit√© √† capturer des relations complexes (notamment sur des r√©seaux profonds).\n",
    "\n",
    "5. **Code** :\n",
    "```python\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "def swish(x): return x * sigmoid(x)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, swish(x), label='Swish')\n",
    "plt.plot(x, relu(x), label='ReLU', linestyle='--')\n",
    "plt.legend(); plt.grid(); plt.title(\"Swish vs ReLU\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295c38b",
   "metadata": {},
   "source": [
    "### üîπ Fonction Softmax ‚Äì R√©ponses\n",
    "\n",
    "1. **Calcul manuel** :\n",
    "   $$\\text{softmax}([1, 2, 3]) = \\left[\n",
    "   \\frac{e^1}{e^1 + e^2 + e^3}, \\frac{e^2}{...}, \\frac{e^3}{...}\n",
    "   \\right] \\approx [0.09, 0.24, 0.67]$$\n",
    "   La somme fait 1.\n",
    "\n",
    "2. **Vecteurs constants** : softmax([100, 100, 100]) = [1/3, 1/3, 1/3] car toutes les exponentielles sont √©gales.\n",
    "\n",
    "3. **Usage** : couche de sortie pour **classification multi-classes**. Interpr√©table comme une distribution de probabilit√©.\n",
    "\n",
    "4. **Diff√©rence avec sigmo√Øde** : la sigmo√Øde s‚Äôapplique ind√©pendamment sur chaque sortie, la softmax consid√®re le **contexte global** du vecteur.\n",
    "\n",
    "5. **Code** :\n",
    "```python\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # stabilit√© num√©rique\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "print(softmax(x))  # [0.09, 0.24, 0.67]\n",
    "print(np.sum(softmax(x)))  # 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818ce14-fdf3-4437-8438-e5a85e93ef60",
   "metadata": {},
   "source": [
    "### üìú Licence d'utilisation\n",
    "Ce notebook est mis √† disposition sous licence **Creative Commons Attribution - Pas d‚ÄôUtilisation Commerciale - Partage dans les M√™mes Conditions 4.0 International**.\n",
    "\n",
    "**Auteur** : Christie Vassilian"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
